# Dataset and Data Collection
Utilising a pre-existing, publicly accessible dataset in this study's data-gathering technique reduces the requirement for primary data collection. The chosen dataset is a CrowdFlower-assembled and disseminated Twitter corpus exclusively discussing self-driving or autonomous vehicles (AVs). This dataset offers a comprehensive and accurate sample of the public's perception of AVs as reflected through social media interactions.
The original dataset included 7015 tweets, each graded on a sentiment scale from 1 to 5. The scores assigned to the tweets range from 1 to 5, with 5 being the most positive attitude and 1 representing the most negative sentiment. The rating method enables a quick, high-level evaluation of the sentiment polarity of each tweet.
The dataset also included a "non-relevant" category, comprising tweets in the corpus but not directly addressing autonomous vehicles. However, the "non-relevant" tweets were left out of the research for the sake of this study, which aims to keep a strong focus on AV-related dialogue. This ensured that all information being considered directly related to the study's goals.
After non-relevant tweets were removed, the final dataset for the study consisted of 6943 tweets, each of which provides a different perspective on the public's perception of AVs. This particular dataset had already been used in a related study by Sadiq & Khan (2022), proving its applicability and usefulness for sentiment analysis in the context of this investigation.
The dataset is freely available for download and can be accessed via the following link:  https://data.world/crowdflower/sentiment-self-driving-cars.

# Loading the Libraries
During my research, I used a wide range of Python packages, each suited to particular aspects of text processing, machine learning, and data analysis. The foundation made it easier to manipulate data and perform numerical operations, such as ‘pandas’ and ‘numpy.’ The ‘nltk’ library's tools for tokenization, stemming, and lemmatization made it vital for natural language processing. Scikit-learn's extensive collection of machine learning algorithms and data vectorization strategies were used, while ‘imblearn’ was used to address the problems presented by unbalanced datasets. The LatentDirichletAllocation class from scikit-learn's decomposition module helped to support topic modelling efforts by making it possible to identify latent topics in enormous amounts of text. Additionally, the ‘wordcloud’ library was used to graphically display the predominance of words in our dataset for visual representation.
# Data Cleaning and Manipulation
In this part, we went into data cleaning and manipulation techniques, highlighting the justification for combining sentiment categories. This was necessary to streamline the dataset, improve model performance, and guarantee clarity in further studies.
The sentiment categories were reorganised for clarity and effectiveness as we refined our data. Both the "Very Negative" and "Negative" categories as well as the "Very Positive" and "Positive" categories were combined. An illustration of this change can be seen in the table below:
Previous values	New Values
1-Very Negative	-1-Negative
2-Negative	
3-Neutral	0-Neutral
4-Positive	
1-Positive
5-Very Positive	
Table 2: Sentiment manipulation 
Reasons for the manipulation:
Five categories may add extra noise and complexity to the following analysis. Sentiment analysis frequently uses the three categories of negative, neutral, and positive since they are more logical. 
Several machine learning models might perform better with fewer classes, particularly if there is a lack of training data for specific classes.
The sentiment categories "very negative" and "very positive" were rare in the original dataset. It will be easier to reduce the consequences of the data imbalance by grouping them with other sentiments that are comparable.
The difference between "negative" and "very negative" (or "positive" and "very positive") may not matter in many situations. The results and visualisations are made easier for a broader audience to understand by integrating them.
# Exploratory Data Analysis
EDA, or exploratory data analysis, was the first phase in the data analysis process. It enabled the researcher to comprehend the data's structure, patterns, anomalies, and other crucial features. The approach used to analyse and visualise the Twitter dataset to acquire insights into its content was described in this part, with a primary emphasis on sentiment analysis.
# Dataset Overview: 
The dataset comprises 6,943 entries divided into two main columns: text and sentiment. The text column contains textual information of the object data type, which denotes strings or textual material. It contains tweets related to Autonomous Vehicles. The sentiment column, on the other hand, represents the sentiment score connected to each text and is kept as an integer. Higher or lower numbers reflect positive or negative sentiments, respectively. Notably, the dataset has no missing values, guaranteeing a thorough analysis without extra imputation.
Index	Text	Sentiment
0	Two places I'd invest all my money if I could:..	1
1	Awesome! Google driverless cars will help the..	1
2	If Google maps can't keep up with road constru..	-1
3	Autonomous cars seem way overhyped given the t..	-1
4	Just saw Google self-driving car on I-34. It w..	0

# Sentiment Distribution: 
The distribution of sentiments within the dataset is presented in the bar chart below. The total number of entries for each sentiment is depicted quantitatively. With 4,245 neutral tweets, it is clear that the bulk of sentiments were impartial. 1,903 tweets were positive, and 795 tweets reflected negative sentiments.  

The study continues to the text normalisation phase, where stemming and lemmatisation will be used after cleaning. The inflectional forms of each word are reduced to a joint base or root using both processes, albeit in different ways and to varying degrees.
Stemming streamlines the normalisation process by only removing a word's affixes without considering its context. When working with vast amounts of data, like in the case of this study, this technique offers the benefit of being less computationally intensive (Singh & Gupta, 2017).
Lemmatisation, on the other hand, employs a sophisticated method that examines the morphological analysis of the words and their context to restore words to their lemma, which is the fundamental form of the dictionary (Balakrishnan & Ethel, 2014). Although this method requires more work than stemming, it yields a more accurate result, which is helpful in some analysis settings.
Both stemming and lemmatisation have advantages in sentiment analysis. Lemmatisation, with its contextual awareness, might result in more accurate representations of the original text, whereas stemming may be able to emphasise sentiment-bearing words in their root forms (Di & Vezzani, n.d.).
Stemming and lemmatisation were used in this dissertation, and their effects on how well sentiment analysis performs were investigated. The study evaluated which methodology delivers more value in this unique research environment by comparing the data produced by the two methodologies. This dual strategy ensured a thorough investigation of text normalisation techniques and their impacts on the following analysis.
Tokenisation, the last stage of data pre-processing, involves dividing the cleaned and normalised text into separate words or tokens (Shyamasundar & Rani, 2016). This crucial step converts the text data into a format that feature extraction and machine learning algorithms can use.
# Feature Extraction
The feature extraction stage is essential for converting the pre-processed text input into a structured numerical representation that machine learning algorithms can comprehend. The Bag-of-Words (BoW) technique of text representation was used in this study comparable to the strategy used by Ding et al. (2021).
The Bag-of-Words model visualises each corpus text as a "bag" or "multiset" of words, ignoring word order but keeping track of word frequency. Each dimension of the high-dimensional vector representing each tweet as a single distinct word from the corpus is thus assigned to a different tweet. The vector elements are determined by the frequency with which each word appears in a tweet (Qader et al., 2019).
The BoW model provides an easy and effective technique to extract features but treats each word equally, which may not always be accurate. The model addresses this by adding the TF-IDF (Term Frequency-Inverse Document Frequency) approach. Higher weights are assigned to terms that are frequent in a document but uncommon in the corpus by TF-IDF, which distributes weights to the words in each document (Yan et al., 2020). By highlighting words that may be more informative, this technique helps eliminate words used frequently throughout all documents.
A document-term matrix is the output of the feature extraction process. Each row represents a document (a tweet), each column represents a term (a word), and each cell represents the TF-IDF weight of the word in the document. The succeeding machine-learning models use this organised format as their input.
We studied the impact of lemmatisation and stemming on the effectiveness of feature extraction at the pre-processing stage, where both were applied. Stemming, which reduced words to their most basic form, could produce words that are not included in dictionaries (for instance, "run" and "running" both reduce to "run"). Despite this, these stemmed words could aid in the relevant feature extraction process when used with the BoW model. However, lemmatisation, which considers the morphological examination of the words, provided greater precision by condensing words to their dictionary base form.
This study ensured that the most informative features were emphasised using the BoW model supplemented with TF-IDF for feature extraction, preparing the data for efficient classification in the model selection step.
# Data Balancing 
The distribution of the dataset across the three sentiment categories was an important factor to consider prior to model training. Initial analysis of the dataset revealed a large imbalance, which frequently results in biased model performance.
To rectify this, we used the data balancing method, the Synthetic Minority Over-sampling Technique (SMOTE). SMOTE and its variations are widely used and effective, and this is well-documented in modern literature. Banerjee et al. (2020) conducted a thorough analysis highlighting the technique's capacity to improve classifier performance. SMOTE, which was first developed by Chawla et al. in 2002, has become a pillar in the effort to remedy dataset imbalance.
In light of these observations and research recommendations, SMOTE was utilised to balance the dataset, ensuring a fairer representation of sentiment categories before continuing with model training.
Partitioning the dataset into training and testing subsets was the crucial next step after resolving the dataset imbalance with SMOTE. This is essential for developing the model and afterwards assessing how well it performs with unseen data.
For training, 80% of the balanced dataset was used, and the other 20% was saved for testing. Setting the ‘test_size’ parameter to 0.2, which designates that 20% of the data is put aside for testing, facilitated this division. The ‘random_state’ option was used and set its value to 42 to assure the consistency and repeatability of our findings. This ensures that the data will be split identically each time the code is run, given the same dataset and random state, maintaining the reliability and consistency of our research.

# Model Selection
We determined which machine learning algorithms were most effective at identifying the feelings in our dataset at this crucial stage of the model selection process. The machine learning models selected for this study were naive Bayes, Support Vector Machines (SVMs), Random Forest, and Deep Learning utilising Long Short-Term Memory (LSTM) networks. The document-term matrix generated during the feature extraction stage served as the input for all of these models, which were chosen owing to their effectiveness in text classification tasks.
# Naïve Bayes
The Naive Bayes technique is a probabilistic model that is frequently used in text classification applications like sentiment analysis. According to Novendri et al. (2020), the algorithm uses word frequencies to calculate the likelihood that a document falls into a particular category. It functions under the feature independence presumption, which states that every word or feature in the dataset is taken to be independent of every other feature. Naive Bayes frequently displays outstanding performance despite its intrinsic simplicity and the "naive" assumption of feature independence, especially in text data situations.
The Multinomial Naive Bayes variation is particularly well-suited for discrete data, in contrast to the regular Naive Bayes classifier, which concentrates on binary or boolean features. This refers to word counts or phrase frequencies in documents in the context of text classification. Multinomial Naive Bayes is frequently more effective than the traditional Naive Bayes model, according to Sharma & Singh (2016), making it the model of choice for tasks like sentiment analysis where phrase frequency is a key consideration.
Due to the Multinomial Naive Bayes classifier's effectiveness in handling phrase frequencies, it was chosen for the research. The process entailed setting up the classifier, using the designated training data to train it, and then using the test dataset to predict sentiments. The effectiveness of these predictions was then evaluated in order to determine the model's accuracy and robustness in identifying the underlying sentiment patterns (refer to appendix 6 for code explanation).
# Support Vector Machines
Support Vector Machines (SVMs) are supervised learning models known for their aptitudes in both regression and classification problems. A key component of SVM is selecting the best hyperplane for clearly categorising data points (Bambrick, 2016). This hyperplane serves as a boundary for decisions. 'Support vectors' are the data points that are located on the margin's edges. These vectors are crucial because they affect how the hyperplane is positioned and oriented. To ensure the best classification, SVM maximises the margin, which is the distance between the hyperplane and the closest data point from either class.
Textual data is inherently high-dimensional, especially when individual words or n-grams are handled as discrete features. Text classification uses SVM to address this issue. In these situations, SVMs excel at managing high-dimensional spaces and preventing overfitting—even when the number of features exceeds the number of samples. When it comes to text categorization, where datasets can contain thousands of different features, this quality is invaluable. SVMs were praised for their effectiveness and resilience in text categorization by D'Andrea et al. (2019) and Niu et al. (2010) (refer to appendix 7 for detailed code explanation).
# Random Forest
The Random Forest classifier is a potent ensemble learning technique that builds many decision trees during training and outputs the categorization of the individual trees' classes for prediction. The Random Forest technique combines the predictions from various decision trees to get a more accurate and reliable prediction as opposed to depending just on one tree. According to Liu et al. (2012), Random Forest is frequently utilised for classification problems.
The following are some advantages associated with Random Forest, as stated by Yu et al. (2017):
	Random Forest is skilled at handling large datasets, making it appropriate for complex datasets with a lot of entries.
	It is effective at decreasing multicollinearity, preventing strongly correlated predictors from having an excessive impact on the model.
	Random Forest is renowned for its resistance to overfitting, which is a frequent error in which models perform well on training data but badly on unobserved data.
	It allows for more flexibility in capturing complicated patterns because it can effectively handle non-linear relationships in the dataset.
To take advantage of these benefits, the Random Forest classifier was set up for this investigation. The Random Forest model was trained during the training phase utilising the carefully selected training data. The model's performance was then assessed using a classification report and predictions made using the test data. 
# Long Short-Term Memory
Long Short-Term Memory (LSTM) networks are a subset of Recurrent Neural Networks (RNNs) that are designed to perform sequence-based tasks. LSTMs, in particular, have demonstrated enhanced sentiment analysis capability by efficiently capturing long-term dependencies and context inside text sequences (Qaisar, 2020).
Enhancements to the Model Architecture
The LSTM architecture was not only used in its vanilla form for the sake of this study, but it was also improved via numerous architectural improvements:
Optimizer: Because of its efficiency and cheap computational requirements, the Adam optimizer was chosen. This optimizer works well for situations with many data and/or parameters.
The function of Loss: The loss function 'sparse_categorical_crossentropy' was chosen since it is well-suited for multi-class classification tasks.
Dense layers: Two Dense layers with 32 units (with 'ReLU' activation) and 3 units (with 'Softmax' activation) were added to the architecture. The first is an intermediate feature extractor, while the latter is a three-class classification output layer.	
Comparative Evaluation
Our LSTM model was tested against established machine learning algorithms to demonstrate its superior performance. The addition of an Attention mechanism improved its classification accuracy even further, correlating with the findings of Liang et al. (2019).
This study attempts to create a stable, trustworthy, and high-performing baseline model for sentiment analysis on text datasets by employing a complex LSTM architecture reinforced with cutting-edge upgrades (Longpre et al., 2016). This methodology demonstrates the study's dedication to a careful and detailed investigation of sentiment categorization, employing various models spanning from classic machine learning techniques to advanced deep learning algorithms.
# Topic Modelling 
In analysing our Twitter corpus, topic modelling has proven vital for revealing latent subject structures in vast volumes of text. One of the most popular methods for topic modelling, Latent Dirichlet Allocation (LDA), will be used in this study (Kim et al., 2016).
A generative probabilistic model called LDA is based on the premise that each document in a corpus is a composite of a fixed number of topics. Each topic is viewed as a distribution over words in turn. With these presumptions, LDAs can identify the text's underlying semantic structures and assign topics to individual documents and the words that make up those documents.
LDA will be applied to the document-term matrix obtained during the feature extraction stage. This matrix gives the LDA a structured input and captures the frequency of phrases used in each document. The programme then finds groups of words (topics) that often recur in documents.
The LDA process results in a list of topics, each represented as a group of words, along with the relevance or weight assigned to each word inside each topic. The distribution of subjects is also given to each document in the corpus, reflecting the prevalence of each topic in the text.
The number of topics strongly influences the model's performance. According to Lugmayr and Grueblbauer (2017), less than ten topics would not adequately capture all the material, while too many subjects, as per Nikolenko et al. (2016), could lead to unpredictable analyses and messy findings. To maximise model performance, this work sets the topic count at 25, in keeping with past studies on tweets or microblogs (Ma et al., 2013; Zhou et al., 2016), however, as our dataset is relatively small, we will keep the topic count at 5.
A reliable and effective way to identify the Twitter collection's dominant themes is to use LDA for topic modelling. By revealing these latent issues, the survey can offer deeper insights into what specific aspects of autonomous vehicles the public is talking about. This adds important context to the sentiment analysis and gives us a complete picture of how people feel about autonomous vehicles. For this reason, it was used in this dissertation to collect the most frequent topics discussed regarding Avs).
# Model Evaluation
The performance of the models, which included Random Forest, Multinomial Naive Bayes, Support Vector Machine and Long Short-Term Memory (LSTM) networks, was evaluated using various evaluation criteria. These measures were carefully chosen to provide a comprehensive, multidimensional view of the model's performance, ensuring a nuanced understanding of its capabilities and limitations. This study measures accuracy, precision, recall, and F1 score.
## Accuracy:
Accuracy is a key indicator of a model's overall predictive capacity. It is determined by dividing the number of accurately predicted cases by the total number of cases in the dataset. While it provides a quick glimpse of performance, it should be read in conjunction with other measures, particularly when the class distribution is skewed.
Mathematically, 	 Accuracy=(True Positives + True Negatives)/(Total Observations)

## Precision:
Precision is a measure of a model's ability to detect positive cases correctly. It measures the fraction of true positive predictions made by the model out of all positive predictions made by the model. High precision suggests a low rate of false-positive mistakes, which is critical when false positives are costly.
Mathematically, 	Precision=(True Positives)/(True Positives+False positives)
 
## Recall:
The ability of a model to identify all relevant instances in a dataset is quantified by the recall, which is also known as sensitivity or the true positive rate. It is calculated by dividing the number of true positives by the total number of true positives and false negatives. Recall is especially critical in applications where missing a positive occurrence has serious effects.
Mathematically, 	Recall=(True Positives)/(True Positives+False Negatives)

## F1- Score:
The F1 Score is a statistic that balances the trade-off between precision and recall by acting as a harmonic mean of these two dimensions. An F1 score is best when it is 1 (perfect precision and recall) and worst when it is 0. It is a helpful measure when the class distribution is skewed and both false positives and false negatives are costly.
Mathematically,	F1-Score=2*((Precision*Recall)/(Precision + Recall))
 
# Interpretations and Implications
Each of the aforementioned measures provides a unique perspective on the model's performance. A high accuracy combined with a low precision, for example, may imply that the model is unsuitable for situations where false positives are costly. A model with high recall but low precision, on the other hand, may be good for preliminary screening but inappropriate for ultimate decision-making.
The interaction of these indicators provides vital insights into the model's capabilities, leading future research, informing model revisions, and laying the groundwork for practical implementations.

